{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from src import visualization, data_loader, filters, features, config\n",
    "\n",
    "visualization.set_plot_style()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Window sizes to test (10ms to 200ms)\n",
    "WINDOW_SIZES_MS = range(10, 201, 10)\n",
    "\n",
    "# Paths\n",
    "TRAINING_DIR = Path(\"../data/labeled_training_data\")\n",
    "TEST_DIR = Path(\"../data/test_data\")\n",
    "RESULTS_DIR = Path(\"../results/thresholds\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training Data: {TRAINING_DIR.resolve()}\")\n",
    "print(f\"Test Data:     {TEST_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(feature_values, true_labels):\n",
    "    \"\"\"Grid search to find the threshold with maximum accuracy.\"\"\"\n",
    "    if len(feature_values) == 0: return 0.0\n",
    "    \n",
    "    # Scan range between min and max feature values\n",
    "    min_v, max_v = np.min(feature_values), np.max(feature_values)\n",
    "    candidates = np.linspace(min_v, max_v, 100)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_thresh = candidates[0]\n",
    "    \n",
    "    for thresh in candidates:\n",
    "        preds = (feature_values >= thresh).astype(int)\n",
    "        acc = accuracy_score(true_labels, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_thresh = thresh\n",
    "    return best_thresh\n",
    "\n",
    "def calculate_detection_delay(y_true, y_pred, fs):\n",
    "    \"\"\"Calculates average delay (ms) from onset (0->1) to first detection.\"\"\"\n",
    "    delays = []\n",
    "    in_event = False\n",
    "    start_idx = None\n",
    "    \n",
    "    for i in range(1, len(y_true)):\n",
    "        # Label Onset\n",
    "        if y_true[i-1] == 0 and y_true[i] == 1:\n",
    "            in_event = True\n",
    "            start_idx = i\n",
    "        \n",
    "        # First Detection\n",
    "        if in_event and y_pred[i] == 1:\n",
    "            delays.append((i - start_idx) / fs * 1000.0)\n",
    "            in_event = False\n",
    "            \n",
    "    return np.nanmean(delays) if delays else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = []\n",
    "feature_funcs = {'RMS': features.calculate_rms, 'VAR': features.calculate_var, 'WL': features.calculate_wl}\n",
    "\n",
    "# 1. Iterate over all training files\n",
    "train_files = list(TRAINING_DIR.glob(\"*.csv\"))\n",
    "if not train_files:\n",
    "    print(\"ERROR: No training files found! Please populate data/labeled_training_data/\")\n",
    "else:\n",
    "    print(f\"Training on {len(train_files)} datasets...\")\n",
    "\n",
    "    for file_path in train_files:\n",
    "        # Load & Preprocess\n",
    "        df = data_loader.load_labeled_csv(file_path)\n",
    "        if df is None: continue\n",
    "            \n",
    "        # Convert to Volts & Center\n",
    "        raw = (df['RawValue'] / 1023.0 * config.V_REF).values\n",
    "        sig = raw - np.mean(raw)\n",
    "        labels = df['LabelNumeric'].values\n",
    "        \n",
    "        # Filter\n",
    "        clean_sig = filters.apply_butterworth_sos(sig, order=4, fs=config.FS)\n",
    "        \n",
    "        # 2. Test all Window Sizes & Features\n",
    "        for win_ms in WINDOW_SIZES_MS:\n",
    "            samples = int(win_ms * config.FS / 1000)\n",
    "            if len(clean_sig) < samples: continue\n",
    "            \n",
    "            target_labels = labels[samples-1:]\n",
    "            \n",
    "            for fname, ffunc in feature_funcs.items():\n",
    "                feats = ffunc(clean_sig, samples)\n",
    "                \n",
    "                # Find optimal threshold for this specific file/window/feature\n",
    "                optimal_thresh = find_best_threshold(feats, target_labels)\n",
    "                \n",
    "                training_results.append({\n",
    "                    \"Dataset\": file_path.name,\n",
    "                    \"Feature\": fname,\n",
    "                    \"WindowSize(ms)\": win_ms,\n",
    "                    \"Threshold\": optimal_thresh\n",
    "                })\n",
    "\n",
    "    # 3. Aggregate Results (Median across datasets)\n",
    "    df_train = pd.DataFrame(training_results)\n",
    "    \n",
    "    # Group by Feature + WindowSize and take the Median threshold\n",
    "    trained_thresholds = df_train.groupby([\"Feature\", \"WindowSize(ms)\"])[\"Threshold\"].median().reset_index()\n",
    "    \n",
    "    # Save to CSV\n",
    "    out_path = RESULTS_DIR / \"trained_thresholds.csv\"\n",
    "    trained_thresholds.to_csv(out_path, index=False)\n",
    "    print(f\"Training Complete. Optimized thresholds saved to:\\n{out_path}\")\n",
    "    \n",
    "    # Show preview\n",
    "    print(trained_thresholds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "\n",
    "# 1. Load the Trained Thresholds\n",
    "if 'trained_thresholds' not in locals():\n",
    "    trained_thresholds = pd.read_csv(RESULTS_DIR / \"trained_thresholds.csv\")\n",
    "\n",
    "# Helper to look up threshold\n",
    "def get_trained_threshold(feat, win):\n",
    "    row = trained_thresholds[\n",
    "        (trained_thresholds[\"Feature\"] == feat) & \n",
    "        (trained_thresholds[\"WindowSize(ms)\"] == win)\n",
    "    ]\n",
    "    return row[\"Threshold\"].values[0] if not row.empty else None\n",
    "\n",
    "# 2. Iterate over Test Files\n",
    "test_files = list(TEST_DIR.glob(\"*.csv\"))\n",
    "if not test_files:\n",
    "    print(\"ERROR: No test files found! Please populate data/test_data/\")\n",
    "else:\n",
    "    print(f\"Evaluating on {len(test_files)} datasets...\")\n",
    "\n",
    "    for file_path in test_files:\n",
    "        # Load & Preprocess\n",
    "        df = data_loader.load_labeled_csv(file_path)\n",
    "        if df is None: continue\n",
    "            \n",
    "        raw = (df['RawValue'] / 1023.0 * config.V_REF).values\n",
    "        sig = raw - np.mean(raw)\n",
    "        labels = df['LabelNumeric'].values\n",
    "        clean_sig = filters.apply_butterworth_sos(sig, order=4, fs=config.FS)\n",
    "        \n",
    "        # 3. Apply Learned Thresholds\n",
    "        for win_ms in WINDOW_SIZES_MS:\n",
    "            samples = int(win_ms * config.FS / 1000)\n",
    "            if len(clean_sig) < samples: continue\n",
    "            \n",
    "            target_labels = labels[samples-1:]\n",
    "            \n",
    "            for fname, ffunc in feature_funcs.items():\n",
    "                # Retrieve the generic threshold learned in Phase 1\n",
    "                thresh = get_trained_threshold(fname, win_ms)\n",
    "                if thresh is None: continue\n",
    "                \n",
    "                # Calculate Feature\n",
    "                feats = ffunc(clean_sig, samples)\n",
    "                \n",
    "                # Make Prediction\n",
    "                preds = (feats >= thresh).astype(int)\n",
    "                \n",
    "                # Score Performance\n",
    "                acc = accuracy_score(target_labels, preds)\n",
    "                delay = calculate_detection_delay(target_labels, preds, config.FS)\n",
    "                \n",
    "                evaluation_results.append({\n",
    "                    \"Dataset\": file_path.name,\n",
    "                    \"Feature\": fname,\n",
    "                    \"WindowSize(ms)\": win_ms,\n",
    "                    \"Threshold\": thresh,\n",
    "                    \"Accuracy\": acc * 100, # Convert to %\n",
    "                    \"DetectionDelay(ms)\": delay\n",
    "                })\n",
    "\n",
    "    # 4. Save Final Results\n",
    "    df_eval = pd.DataFrame(evaluation_results)\n",
    "    out_path = RESULTS_DIR / \"evaluation_from_training.csv\"\n",
    "    df_eval.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"Evaluation Complete. Full results saved to:\\n{out_path}\")\n",
    "    print(f\"You can now run Notebook 06 to visualize these statistics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
